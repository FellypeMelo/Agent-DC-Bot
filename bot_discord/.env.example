# Configurações do Discord
DISCORD_TOKEN=seu_token_aqui
BOT_PREFIX=!

# LLM Backend Selection
# Opcoes: llama_cpp (auto-start), lm_studio (standard), ollama
LLM_BACKEND=llama_cpp

# --- Configurações do Llama.cpp (llama-server.exe) ---
# Usado quando LLM_BACKEND=llama_cpp. O bot inicia o servidor automaticamente.
LLAMA_SERVER_PATH=./llama-server.exe
LLAMA_SERVER_HOST=127.0.0.1
LLAMA_SERVER_PORT=8080
LLAMA_SERVER_FLAGS=-c 4096 -ngl 33
LLM_MODEL_PATH=C:/path/to/model.gguf

# --- Configurações do LM Studio (Padrao) ---
# Usado quando LLM_BACKEND=lm_studio. Requer que o LM Studio esteja aberto.
LM_STUDIO_API_URL=http://localhost:1234/v1
AI_MODEL=ministral-3:3b

# --- Configurações do Ollama ---
# Usado quando LLM_BACKEND=ollama. Requer que o Ollama esteja rodando.
OLLAMA_API_URL=http://localhost:11434/v1
OLLAMA_MODEL=ministral-3:3b

# Configurações de Busca
SEARCH_ENABLED=true
CACHE_ENABLED=true
CACHE_EXPIRY=24

# Moderação Automática
MODERATION_ENABLED=false

# Log Level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
