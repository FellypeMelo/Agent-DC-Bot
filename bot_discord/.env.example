# Configurações do Discord
DISCORD_TOKEN=seu_token_aqui
BOT_PREFIX=!

# LLM Backend Selection (Se todos false, usa LM Studio)
USE_LOCAL_LLM=false
USE_OLLAMA=false

# Configurações do LM Studio (Padrao)
LM_STUDIO_API_URL=http://localhost:1234/v1
AI_MODEL=ministral-3:3b

# Configurações do Ollama
OLLAMA_API_URL=http://localhost:11434/v1
OLLAMA_MODEL=ministral-3:3b
LOCAL_MODEL_PATH=C:/path/to/model.gguf
GPU_LAYERS=-1

# Configurações do LM Studio
LM_STUDIO_API_URL=http://localhost:1234/v1
AI_MODEL=gemma-2b

# Configurações de Busca
SEARCH_ENABLED=true
CACHE_ENABLED=true
CACHE_EXPIRY=24

# Moderação Automática
MODERATION_ENABLED=false

# Log Level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Configurações do Llama.cpp (llama-server.exe)
LLM_BACKEND=llama_cpp
LLAMA_SERVER_PATH=./llama-server.exe
LLAMA_SERVER_HOST=127.0.0.1
LLAMA_SERVER_PORT=8080
LLAMA_SERVER_FLAGS=-c 4096 -ngl 33
